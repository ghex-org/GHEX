MPI_THREAD_MULTIPLE

BASELINE: 1 thread

-M multi is important, because we want to use MT version of UCX. This adds overhead
time numactl --physcpubind=0-15 ucx_perftest -t tag_bw -M multi c3-4 -n 5000000 -s 1 -O 1

---------------------------------------------------------------------
niters			5000000
msg_size		1	1	1	1024	1024
inflight  		1	10	1000	1	10
---------------------------------------------------------------------

ucx_perftest		0.63	0.63		1.55	1.55

------------------------------
OpenMPI + HPCX 2.4
------------------------------
mpi_avail (waitany)   	1.40	2.10	80	1.78	2.51
mpi_avail (testany)   	0.86	0.98	13	1.63	1.57
mpi_avail_iter		0.82	0.85	1.18	1.83	1.71

------------------------------
Intel MPI  - even worse than the results below
------------------------------
mpi_avail (waitany)   	2.07	2.08	20	2.49	2.30
mpi_avail (testany)   	2.44	2.65	29	2.86	
mpi_avail_iter 		2.39	2.42	2.60	2.80	6.7

------------------------------
ghex_futures.cpp
------------------------------
MPI (ompi)  		0.85	0.86	1.21	1.59	1.75
MPI (intel)		
UCX			0.71	0.71	0.86	1.50	1.61

------------------------------
ghex_msg_cb.cpp
------------------------------
Sending a shared_message using callbacks.
n requests are submited and completed in turns.
Both backends can be used.

MPI (ompi)  		1.42	1.43	2.15	2.11	2.33
MPI (intel)		2.93	2.96	8.14	3.32	9.82
UCX			0.72	0.73	1.00	1.47	1.65
UCX nbr/ghex progress	1.33	1.31	1.47	2.21	2.31

------------------------------
ghex_msg_cb_avail.cpp
------------------------------
Sending a shared_message using callbacks. There is n in-flight requests.
Messages are sent as slots become available, i.e., requests are completed.
Both backends can be used.

MPI (ompi)  		1.48	1.43	2.38	2.26	2.32
MPI (intel)		3.01	3.05	-	3.42   13.10
UCX			0.85	0.73	1.03	1.65	1.50
UCX nbr/ghex progress	1.34	1.31	1.52	2.31	2.29

------------------------------
ghex_msg_cb_resubmit.cpp
------------------------------
Sending a shared_message using callbacks. There is n in-flight requests.
Messages are sent as slots become available, i.e., requests are completed.
recv requests are resubmited inside the recv callback.
Both backends can be used.

MPI (ompi)  		1.45	1.44	2.15	2.35	2.33
MPI (intel)		3.02	3.07	9.47	3.46   13.20	
UCX			0.83	0.73	1.40	1.48	1.56  (new MsgType overhead for large inflight)
UCX nbr/ghex progress	1.38	1.32	1.55	2.22	2.38

------------------------------
ghex_ptr_cb.cpp
------------------------------
Sending buffer directly through pointer, using callbacks.
n requests are submited and completed in turns.
only UCX

UCX			0.71	0.72	1.06    1.61	1.58

------------------------------
ghex_ptr_cb_avail.cpp
------------------------------
Sending buffer directly through pointer, using callbacks.
There is n in-flight requests.
Messages are sent as slots become available, i.e., requests are completed.
only UCX

UCX			0.83	0.72	1.07	1.61	1.52
